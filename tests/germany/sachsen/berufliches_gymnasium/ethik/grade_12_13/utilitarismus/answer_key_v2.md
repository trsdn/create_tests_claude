---
title: "Erwartungshorizont und Lösungen: Stundenarbeit V2 Handlungsutilitarismus vs. Regelutilitarismus"
subject: Ethik/Philosophie
country: Germany
bundesland: Sachsen
school_type: Berufliches Gymnasium
klassenstufe: 12/13
document_type: "Lehrerkopie / Answer Key"
test_version: "v2"
language: de
created_by: Test Designer Agent
date_created: 2025-11-20
---

# Erwartungshorizont und Musterlösungen

## Stundenarbeit V2: Handlungsutilitarismus vs. Regelutilitarismus

**Berufliches Gymnasium Sachsen | Ethik/Philosophie | Klassenstufe 12/13**

**Lehrerkopie - Vertraulich**

---

## Allgemeine Bewertungshinweise

### Bewertungskriterien

**Inhaltliche Qualität (60-70%):**
- Fachliche Korrektheit der Darstellung
- Vollständigkeit der Argumentation
- Korrekte Verwendung philosophischer Fachbegriffe
- Systematische Anwendung der ethischen Theorien

**Formale Qualität (20-30%):**
- Strukturierte und klare Darstellung
- Logischer Argumentationsaufbau
- Sprachliche Präzision

**Reflexionstiefe (10-20%):**
- Kritisches Denken
- Eigenständige Gedankenführung
- Differenzierte Betrachtung

### Zeitmanagement-Hinweis

Bei 45 Minuten Bearbeitungszeit sollten Schüler etwa:
- **Aufgabe 1:** 12 Minuten (12 Punkte)
- **Aufgabe 2:** 12 Minuten (12 Punkte)
- **Aufgabe 3:** 15 Minuten (15 Punkte)
- **Aufgabe 4:** 6 Minuten (6 Punkte)

---

## Aufgabe 1: Analyse aus handlungsutilitaristischer Perspektive (12 Punkte)

### Aufgabe 1a: Grundprinzip des Handlungsutilitarismus (4 Punkte)

**Erwartete Kernaspekte:**
1. **Konsequentialismus:** Handlungen werden ausschließlich nach ihren Folgen bewertet (1 Punkt)
2. **Nutzenmaximierung:** Ziel ist die Maximierung des Gesamtnutzens/Gesamtwohls (1 Punkt)
3. **Einzelfallbetrachtung:** Jede Handlung wird individuell in ihrer konkreten Situation bewertet (1 Punkt)
4. **Entscheidungskriterium:** Die Handlung ist richtig, die in dieser Situation den größten Nutzen für die größte Anzahl bewirkt (1 Punkt)

**Musterlösung:**

Der Handlungsutilitarismus ist eine konsequentialistische Ethik, die Handlungen ausschließlich nach ihren Folgen bewertet. Das Grundprinzip lautet: Eine Handlung ist dann moralisch richtig, wenn sie in der konkreten Situation den größtmöglichen Nutzen bzw. das größte Gesamtwohl für alle Betroffenen bewirkt. Dabei wird jede Handlung einzeln betrachtet und durch systematische Abwägung aller positiven und negativen Konsequenzen beurteilt. Der Handlungsutilitarist fragt: "Welche Handlungsoption führt hier und jetzt zum besten Gesamtergebnis?"

**Bewertungshinweise:**
- 4 Punkte: Alle vier Kernaspekte klar dargestellt
- 3 Punkte: 3 Kernaspekte behandelt, kleinere Ungenauigkeiten
- 2 Punkte: Grundverständnis erkennbar, aber wesentliche Aspekte fehlen
- 0-1 Punkte: Grundprinzip nicht verstanden oder falsch dargestellt

---

### Aufgabe 1b: Anwendung auf das Gedankenexperiment (8 Punkte)

**Erwartete Kernaspekte:**

1. **Systematische Konsequenzenanalyse (3 Punkte):**

   **Option A (Insassenschutz-Modus):**
   - Positive Konsequenzen: Schnelle Marktdurchdringung, ca. 12.000 Verkehrstote in 10 Jahren
   - Negative Konsequenzen: 200 zusätzlich gefährdete Unbeteiligte in Notfallsituationen
   - Gesamtbilanz: 12.200 Opfer

   **Option B (Gesamtnutzen-Modus):**
   - Positive Konsequenzen: Keine zusätzlich gefährdeten Unbeteiligten
   - Negative Konsequenzen: Langsamere Marktdurchdringung, ca. 18.000 Verkehrstote in 10 Jahren
   - Gesamtbilanz: 18.000 Opfer

2. **Nutzenkalkulation (2 Punkte):**
   - Option A: 12.200 Gesamtopfer
   - Option B: 18.000 Gesamtopfer
   - Differenz: Option A rettet netto ca. 5.800 Menschenleben mehr als Option B

3. **Eindeutige handlungsutilitaristische Empfehlung (2 Punkte):**
   Ein Handlungsutilitarist würde **Option A (Insassenschutz-Modus)** empfehlen, weil diese Programmierung das beste Gesamtergebnis erzielt: ca. 5.800 mehr gerettete Leben.

4. **Begründung (1 Punkt):**
   Die zusätzliche Gefährdung von 200 Unbeteiligten wird durch die 5.800 geretteten Leben bei weitem überwogen. Der schnellere Übergang zu autonomen Fahrzeugen maximiert das Gesamtwohl.

**Musterlösung:**

Aus handlungsutilitaristischer Perspektive muss Lisa eine systematische Nutzen-Schaden-Analyse durchführen:

**Option A (Insassenschutz-Modus):** Diese Programmierung führt zu schneller Marktdurchdringung (50% nach 5 Jahren), weil Käufer sich sicher fühlen. In 10 Jahren würden ca. 12.000 Menschen im Straßenverkehr sterben. Allerdings werden in den seltenen Notfallsituationen (0,1%) zusätzlich ca. 200 Unbeteiligte gefährdet. Gesamtbilanz: 12.200 Opfer.

**Option B (Gesamtnutzen-Modus):** Diese Programmierung führt zu langsamer Marktdurchdringung (nur 20% nach 5 Jahren), weil Käufer befürchten, vom eigenen Auto "geopfert" zu werden. In 10 Jahren würden ca. 18.000 Menschen im Straßenverkehr sterben. Es gibt keine zusätzlich gefährdeten Unbeteiligten. Gesamtbilanz: 18.000 Opfer.

**Nutzenkalkulation:**
- Option A: 12.200 Gesamtopfer
- Option B: 18.000 Gesamtopfer
- **Differenz: Option A rettet netto ca. 5.800 Menschenleben**

Der Handlungsutilitarist würde eindeutig **Option A (Insassenschutz-Modus) empfehlen**. Obwohl diese Programmierung in Notfallsituationen 200 Unbeteiligte zusätzlich gefährdet, führt sie durch die schnellere Verbreitung autonomer Fahrzeuge zu einem massiv besseren Gesamtergebnis: 5.800 mehr gerettete Leben in 10 Jahren. Aus rein konsequentialistischer Sicht ist die Programmierung moralisch richtig, die das Gesamtwohl maximiert – und das ist eindeutig Option A.

**Bewertungshinweise:**
- 8 Punkte: Vollständige systematische Analyse, klare Kalkulation, eindeutige Empfehlung mit Begründung
- 6-7 Punkte: Systematische Analyse, kleinere Lücken in Argumentation oder Kalkulation
- 4-5 Punkte: Grundlegende Anwendung erkennbar, aber unvollständig
- 2-3 Punkte: Ansätze vorhanden, aber wesentliche Fehler
- 0-1 Punkte: Handlungsutilitarismus nicht korrekt angewendet

---

## Aufgabe 2: Analyse aus regelutilitaristischer Perspektive (12 Punkte)

### Aufgabe 2a: Grundprinzip des Regelutilitarismus (4 Punkte)

**Erwartete Kernaspekte:**
1. **Regelorientierung:** Nicht einzelne Handlungen, sondern allgemeine Regeln werden nach Konsequenzen bewertet (1 Punkt)
2. **Langfristige Nutzenmaximierung:** Ziel ist, Regeln zu etablieren, die langfristig das größte Gesamtwohl bewirken (1 Punkt)
3. **Unterschied zum Handlungsutilitarismus:** Statt "Was bringt diese Handlung jetzt?" fragt der Regelutilitarist: "Was wäre, wenn alle in solchen Situationen nach dieser Regel handeln würden?" (1 Punkt)
4. **Systemischer Nutzen:** Fokus auf dem Nutzen stabiler Regelsysteme für die Gesellschaft (1 Punkt)

**Musterlösung:**

Der Regelutilitarismus bewertet nicht einzelne Handlungen direkt nach ihren Konsequenzen, sondern etabliert allgemeine Handlungsregeln, die langfristig das größte Gesamtwohl für die Gesellschaft bewirken. Eine Handlung ist dann moralisch richtig, wenn sie einer Regel folgt, deren allgemeine Befolgung den besten Gesamtnutzen hätte. Der entscheidende Unterschied zum Handlungsutilitarismus: Statt zu fragen "Was bringt diese konkrete Handlung hier und jetzt?", fragt der Regelutilitarist: "Was wäre, wenn jeder in solchen Situationen nach dieser Regel handeln würde?" Der Regelutilitarismus betont den systemischen Nutzen stabiler Regeln, die langfristig das gesellschaftliche Wohl maximieren.

**Bewertungshinweise:**
- 4 Punkte: Alle vier Kernaspekte klar dargestellt, Unterschied deutlich
- 3 Punkte: Grundprinzip verstanden, 3 Kernaspekte behandelt
- 2 Punkte: Ansätze erkennbar, aber unvollständig
- 0-1 Punkte: Grundprinzip nicht verstanden

---

### Aufgabe 2b: Anwendung auf Lisas Situation (8 Punkte)

**Erwartete Kernaspekte:**

1. **Identifikation relevanter Regeln (2 Punkte):**
   - **Regel 1:** "Technische Systeme dürfen Unbeteiligte nicht instrumentalisieren"
   - **Regel 2:** "Jeder Mensch hat das gleiche Recht auf Schutz, unabhängig davon, ob er Nutzer oder Unbeteiligter ist"
   - **Regel 3:** "Maschinen dürfen keine Menschen für andere Menschen opfern"

2. **Begründung des langfristigen Gesamtnutzens dieser Regeln (3 Punkte):**

   **Wenn die Regel "Gleichbehandlung aller Menschen" allgemein befolgt wird:**
   - Langfristiges Vertrauen in autonome Technologien bleibt erhalten
   - Gesellschaftlicher Konsens über ethische Grenzen der KI wird etabliert
   - Verhindert gefährlichen Präzedenzfall: Wenn heute Unbeteiligte instrumentalisiert werden dürfen, wo hört das auf?
   - Schutz fundamentaler Menschenrechte (Würde, Gleichheit)

   **Wenn die gegenteilige Regel befolgt würde ("Nutzer dürfen bevorzugt werden"):**
   - Zwei-Klassen-Gesellschaft: Käufer von Technologie sind mehr wert als Nicht-Käufer
   - Erosion des Prinzips der Gleichwertigkeit aller Menschen
   - Langfristig könnten weitere Diskriminierungen gerechtfertigt werden

3. **Regelutilitaristische Empfehlung (2 Punkte):**
   Ein Regelutilitarist würde **Option B (Gesamtnutzen-Modus)** empfehlen, weil die allgemeine Befolgung der Regel "Gleichbehandlung aller Menschen, keine Instrumentalisierung Unbeteiligter" langfristig das größte Gesamtwohl bewirkt.

4. **Begründung mit Fokus auf systemischen Nutzen (1 Punkt):**
   Obwohl kurzfristig mehr Menschen sterben, ist die Wahrung fundamentaler ethischer Prinzipien langfristig wichtiger für das gesellschaftliche Wohl.

**Musterlösung:**

Aus regelutilitaristischer Perspektive muss Lisa fragen: "Welche allgemeinen Regeln sollten für die Programmierung autonomer Systeme gelten, damit langfristig das größte Gesamtwohl bewirkt wird?"

**Relevante Regeln:**
1. "Technische Systeme dürfen Unbeteiligte nicht instrumentalisieren"
2. "Jeder Mensch hat das gleiche Recht auf Schutz, unabhängig vom Nutzerstatus"
3. "Maschinen dürfen keine Menschen für andere Menschen opfern"

**Begründung des langfristigen Nutzens:**

Wenn die Regel "Gleichbehandlung aller Menschen durch autonome Systeme" allgemein befolgt wird, entstehen folgende langfristige positive Konsequenzen:
- Das Vertrauen in autonome Technologien bleibt erhalten, weil klare ethische Grenzen existieren
- Ein gesellschaftlicher Konsens über ethische KI-Prinzipien wird etabliert
- Fundamentale Menschenrechte (Würde, Gleichwertigkeit) werden geschützt
- Es wird ein gefährlicher Präzedenzfall verhindert: Wenn heute Unbeteiligte für Käuferinteressen instrumentalisiert werden dürfen, wo endet diese Logik?

Wenn dagegen die Regel "Käufer/Nutzer dürfen bevorzugt werden" befolgt würde, hätte dies problematische langfristige Folgen:
- Eine Zwei-Klassen-Gesellschaft: Menschen mit Kaufkraft sind mehr wert als andere
- Erosion des fundamentalen Prinzips der Gleichwertigkeit aller Menschen
- Legitimation weiterer Diskriminierungen ("Wer zahlt, wird bevorzugt")
- Langfristiger Vertrauensverlust in KI-Systeme und gesellschaftliche Konflikte

**Regelutilitaristische Empfehlung:**

Ein Regelutilitarist würde **Option B (Gesamtnutzen-Modus)** empfehlen. Obwohl in den nächsten 10 Jahren kurzfristig mehr Menschen sterben (18.000 statt 12.200), bewirkt die allgemeine Befolgung der Regel "Gleichbehandlung aller Menschen, keine Instrumentalisierung Unbeteiligter" langfristig ein deutlich größeres Gesamtwohl. Die Wahrung fundamentaler ethischer Prinzipien, die Vermeidung einer Zwei-Klassen-Gesellschaft und der Schutz des gesellschaftlichen Vertrauens in autonome Systeme überwiegen den kurzfristigen Nachteil. Der Regelutilitarismus opfert nicht grundlegende Menschenrechte für kurzfristige Nutzengewinne.

**Bewertungshinweise:**
- 8 Punkte: Relevante Regeln identifiziert, langfristiger Nutzen überzeugend dargelegt, klare Empfehlung
- 6-7 Punkte: Regeln identifiziert, langfristiger Nutzen dargelegt, kleinere Lücken
- 4-5 Punkte: Grundlegende Anwendung erkennbar, unvollständige Regelanalyse
- 2-3 Punkte: Ansätze vorhanden, aber wesentliche Fehler
- 0-1 Punkte: Regelutilitarismus nicht korrekt angewendet

---

## Aufgabe 3: Vergleichende Gegenüberstellung und kritische Bewertung (15 Punkte)

### Aufgabe 3a: Unterschiedliche Entscheidungen und zentraler Konflikt (7 Punkte)

**Erwartete Inhalte:**

1. **Klare Benennung der unterschiedlichen Empfehlungen (2 Punkte):**
   - Handlungsutilitarismus: Option A (Insassenschutz-Modus)
   - Regelutilitarismus: Option B (Gesamtnutzen-Modus)

2. **Zentraler Konflikt (3 Punkte):**
   - **Kurzfristige vs. langfristige Konsequenzen:** Handlungsutilitarismus fokussiert auf das beste Ergebnis in 10 Jahren (12.200 vs. 18.000 Opfer), Regelutilitarismus auf langfristige systemische Effekte (Menschenrechte, Vertrauen)
   - **Quantifizierbare vs. nicht-quantifizierbare Werte:** Handlungsutilitarismus zählt Leben, Regelutilitarismus berücksichtigt schwer messbare Werte wie Gleichwertigkeit, Würde, Vertrauen
   - **Instrumentalisierung vs. Deontologie:** Handlungsutilitarismus erlaubt Instrumentalisierung Unbeteiligter für größeres Wohl, Regelutilitarismus setzt absolute Grenzen

3. **Erklärung der unterschiedlichen Ergebnisse (2 Punkte):**
   Die Ansätze unterscheiden sich in:
   - **Zeitperspektive:** Kurzfristig (10 Jahre) vs. langfristig (Generationen)
   - **Bewertungsebene:** Konkrete Programmierung vs. allgemeine Regel für KI-Ethik
   - **Konsequenztypen:** Direkte Todesfälle vs. systemische Effekte (Vertrauen, Rechte)

**Musterlösung:**

Die beiden utilitaristischen Ansätze führen zu entgegengesetzten Empfehlungen:

- **Handlungsutilitarismus:** Option A (Insassenschutz-Modus), weil diese Programmierung in den nächsten 10 Jahren 5.800 mehr Leben rettet (12.200 vs. 18.000 Opfer).
- **Regelutilitarismus:** Option B (Gesamtnutzen-Modus), weil die Regel "Gleichbehandlung aller Menschen" langfristig das größte Gesamtwohl bewirkt.

**Zentraler Konflikt:**

Der Kern des Konflikts liegt im Spannungsverhältnis zwischen kurzfristigem, quantifizierbarem Nutzen und langfristiger, systemischer Stabilität. Der Handlungsutilitarismus priorisiert das unmittelbar messbare Ergebnis (5.800 gerettete Leben in 10 Jahren) und ist bereit, dafür Unbeteiligte zu instrumentalisieren. Der Regelutilitarismus beharrt darauf, dass fundamentale ethische Prinzipien wie die Gleichwertigkeit aller Menschen nicht für kurzfristige Vorteile aufgegeben werden dürfen.

Ein zweiter Konflikt: Der Handlungsutilitarismus konzentriert sich auf quantifizierbare Konsequenzen (Anzahl der Toten), während der Regelutilitarismus auch schwer messbare, aber langfristig wichtige Werte berücksichtigt (Vertrauen in KI, gesellschaftlicher Konsens über Menschenrechte, Verhinderung einer Zwei-Klassen-Gesellschaft).

**Warum unterschiedliche Ergebnisse?**

Die Ansätze unterscheiden sich fundamental in drei Dimensionen:

1. **Zeitperspektive:** Der Handlungsutilitarismus betrachtet die nächsten 10 Jahre, der Regelutilitarismus denkt in Generationen und systemischen Langzeiteffekten.

2. **Bewertungsebene:** Der Handlungsutilitarismus bewertet diese konkrete Programmierung isoliert ("Was ist das beste Ergebnis?"), der Regelutilitarismus bewertet die allgemeine Regel für KI-Ethik ("Welche Regel sollte für alle autonomen Systeme gelten?").

3. **Konsequenztypen:** Der Handlungsutilitarismus fokussiert auf direkte, quantifizierbare Konsequenzen (Todesfälle), der Regelutilitarismus berücksichtigt auch indirekte, systemische Konsequenzen (Erosion von Menschenrechten, Vertrauensverlust, gesellschaftliche Spaltung).

**Bewertungshinweise:**
- 7 Punkte: Unterschiedliche Empfehlungen klar benannt, zentraler Konflikt präzise herausgearbeitet, Erklärung überzeugend
- 5-6 Punkte: Unterschiedliche Empfehlungen benannt, Konflikt erkannt, Erklärung teilweise gelungen
- 3-4 Punkte: Unterschiede erkannt, aber oberflächliche Darstellung
- 0-2 Punkte: Wesentliche Fehler oder unzureichende Darstellung

---

### Aufgabe 3b: Stärken und Schwächen (8 Punkte)

**Erwartete Inhalte (je 2 Punkte pro Stärke/Schwäche):**

**Handlungsutilitarismus:**

**Stärke (2 Punkte):**
- **Pragmatismus und quantifizierbare Ergebnisse:** Der Handlungsutilitarismus bietet klare, messbare Kriterien (5.800 gerettete Leben). Er fokussiert auf das, was konkret zählt: Menschenleben retten. In diesem Fall führt die Rechnung zu einem eindeutigen, nachvollziehbaren Ergebnis, das das größte unmittelbare Leid verhindert.

**Schwäche (2 Punkte):**
- **Instrumentalisierung und Erosion fundamentaler Rechte:** Der Handlungsutilitarismus erlaubt die Instrumentalisierung von 200 Unbeteiligten für das größere Wohl. Dies verletzt das Prinzip der Gleichwertigkeit aller Menschen und könnte einen gefährlichen Präzedenzfall schaffen: Wenn heute Unbeteiligte für Käuferinteressen instrumentalisiert werden dürfen, legitimiert das zukünftig weitere Diskriminierungen. Langfristig könnte dies das gesellschaftliche Vertrauen in autonome Systeme untergraben.

**Regelutilitarismus:**

**Stärke (2 Punkte):**
- **Schutz fundamentaler Werte und langfristige Stabilität:** Der Regelutilitarismus wahrt fundamentale ethische Prinzipien wie die Gleichwertigkeit aller Menschen. Er verhindert eine gefährliche Abwärtsspirale, in der Menschen zunehmend nach Kaufkraft oder Nutzerstatus bewertet werden. Langfristig schafft dies Vertrauen in autonome Technologien und gesellschaftlichen Konsens über ethische KI-Grenzen.

**Schwäche (2 Punkte):**
- **Mangelnde Flexibilität und hohe kurzfristige Kosten:** Der Regelutilitarismus führt in diesem Fall dazu, dass 5.800 Menschen mehr sterben müssen. Die starre Anwendung der Regel "Gleichbehandlung" erscheint angesichts dieser massiven Differenz unnachgiebig. Die moralische Intuition vieler Menschen würde fragen: Müssen wirklich 5.800 Menschen sterben, um ein abstraktes Prinzip zu wahren?

**Musterlösungen:**

**Handlungsutilitarismus:**

**Stärke:** Der Handlungsutilitarismus bietet einen klaren, pragmatischen Ansatz mit quantifizierbaren Ergebnissen. In diesem Fall ist die Rechnung eindeutig: Option A rettet 5.800 mehr Leben in 10 Jahren. Der Ansatz fokussiert auf das, was konkret am wichtigsten ist – Menschenleben retten – und vermeidet starre Regelanwendungen, die zu massivem vermeidbarem Leid führen würden.

**Schwäche:** Der Handlungsutilitarismus erlaubt die Instrumentalisierung von 200 Unbeteiligten für das größere Wohl. Dies verletzt fundamentale Prinzipien wie die Gleichwertigkeit aller Menschen und schafft einen gefährlichen Präzedenzfall: Eine Gesellschaft, in der Käufer/Nutzer mehr wert sind als Unbeteiligte. Langfristig könnte dies zu einer Erosion von Menschenrechten und Vertrauensverlust in autonome Systeme führen.

**Regelutilitarismus:**

**Stärke:** Der Regelutilitarismus wahrt fundamentale ethische Prinzipien wie die Gleichwertigkeit aller Menschen und verhindert deren Instrumentalisierung. Er berücksichtigt nicht nur quantifizierbare kurzfristige Effekte, sondern auch langfristige systemische Konsequenzen wie gesellschaftliches Vertrauen in KI, Konsens über ethische Grenzen und Schutz der Menschenwürde – Werte, die für die Gesellschaft essentiell sind.

**Schwäche:** Der Regelutilitarismus führt in diesem extremen Fall zu 5.800 zusätzlichen Todesfällen. Die starre Anwendung der Regel "Gleichbehandlung" erscheint angesichts dieser massiven Differenz unnachgiebig und schwer zu rechtfertigen. Die moralische Intuition fragt: Müssen wirklich fast 6.000 Menschen sterben, um ein abstraktes Prinzip zu wahren, das langfristig vielleicht nicht einmal den behaupteten Nutzen bringt?

**Bewertungshinweise:**
- 8 Punkte: Alle vier Aspekte (je 1 Stärke + 1 Schwäche pro Ansatz) klar benannt und überzeugend am Beispiel erläutert
- 6-7 Punkte: Stärken und Schwächen benannt, Erläuterung teilweise gelungen
- 4-5 Punkte: Stärken/Schwächen genannt, aber wenig Bezug zum konkreten Fall
- 0-3 Punkte: Wesentliche Fehler oder unzureichende Darstellung

---

## Aufgabe 4: Eigene philosophisch begründete Position (6 Punkte)

**Hinweis:** Diese Aufgabe bewertet nicht die inhaltliche Position, sondern die **Qualität der philosophischen Begründung**.

**Bewertungskriterien:**

1. **Klare Positionierung (2 Punkte):**
   - Eindeutige Empfehlung (Option A oder B)
   - Explizite Stellungnahme, welcher Ansatz überzeugender ist

2. **Philosophische Begründung (2 Punkte):**
   - Differenzierte Abwägung
   - Berücksichtigung verschiedener ethischer Perspektiven
   - Reflexion über Grenzen und Stärken der Ansätze

3. **Eigenständigkeit (2 Punkte):**
   - Nicht bloße Wiederholung
   - Eigenständige Gedankenführung
   - Kritische Reflexion

**Mögliche Argumentationslinien (Beispiele):**

**Position A: Handlungsutilitarismus / Option A**

"Ich würde Lisa Option A (Insassenschutz-Modus) empfehlen, weil der Handlungsutilitarismus in diesem extremen Fall überzeugender ist. Die Differenz von 5.800 geretteten Leben in 10 Jahren ist so massiv, dass sie die Bedenken bezüglich der Instrumentalisierung von 200 Unbeteiligten überwiegt. Allerdings sollte zusätzlich eine gesellschaftliche Debatte über KI-Ethik geführt und transparente Regelungen geschaffen werden, um langfristige systemische Risiken zu minimieren. Aus verantwortungsethischer Perspektive (Jonas) trägt Lisa Verantwortung für die konkreten 5.800 Menschen, die ohne Option A sterben würden."

**Position B: Regelutilitarismus / Option B**

"Ich würde Lisa Option B (Gesamtnutzen-Modus) empfehlen, weil der regelutilitaristische Ansatz überzeugender ist. Fundamentale Menschenrechte wie die Gleichwertigkeit aller Menschen dürfen nicht für kurzfristige Vorteile aufgegeben werden. Aus deontologischer Perspektive (Kant) ist die Instrumentalisierung von Menschen als bloße Mittel moralisch verwerflich. Zudem ist fraglich, ob die Zahlen des Handlungsutilitarismus verlässlich sind – langfristig könnte Vertrauensverlust in KI die Marktdurchdringung auch bei Option A bremsen."

**Position C: Differenzierte Position**

"Beide Ansätze greifen zu kurz. Option A rettet kurzfristig mehr Leben, verletzt aber fundamentale Prinzipien. Option B wahrt Prinzipien, kostet aber 5.800 Leben. Ich würde einen Mittelweg empfehlen: Option B programmieren, aber gleichzeitig massive öffentliche Kampagnen starten, um Vertrauen in autonome Fahrzeuge aufzubauen und die Marktdurchdringung zu beschleunigen. Aus tugendethischer Perspektive wäre dies der Ansatz praktischer Weisheit (phronesis): Prinzipientreue und pragmatische Lösungen verbinden."

**Bewertungshinweise:**
- 6 Punkte: Klare Position, philosophisch fundierte Begründung, eigenständiges Denken, verschiedene Perspektiven
- 4-5 Punkte: Klare Position, Begründung vorhanden, ausbaufähig in Tiefe
- 2-3 Punkte: Position bezogen, aber oberflächliche Begründung
- 0-1 Punkte: Keine klare Position oder keine Begründung

---

## Gesamtbewertung

**Punkteverteilung:**

| Aufgabe | Maximale Punktzahl |
|---------|-------------------|
| Aufgabe 1a | 4 Punkte |
| Aufgabe 1b | 8 Punkte |
| Aufgabe 2a | 4 Punkte |
| Aufgabe 2b | 8 Punkte |
| Aufgabe 3a | 7 Punkte |
| Aufgabe 3b | 8 Punkte |
| Aufgabe 4 | 6 Punkte |
| **GESAMT** | **45 Punkte** |

---

## Zeitmanagement-Validierung für 45 Minuten

**Aufgabe 1 (12 Punkte / ~12 Minuten):**
- 1a: Grundprinzip erläutern (4 Sätze) → 3 Minuten
- 1b: Anwendung mit Kalkulation (10 Zeilen) → 9 Minuten
- **Realistisch: Ja**

**Aufgabe 2 (12 Punkte / ~12 Minuten):**
- 2a: Grundprinzip und Unterschied (5 Zeilen) → 3 Minuten
- 2b: Anwendung mit Regeln (10 Zeilen) → 9 Minuten
- **Realistisch: Ja**

**Aufgabe 3 (15 Punkte / ~15 Minuten):**
- 3a: Vergleich und Konflikt (9 Zeilen) → 8 Minuten
- 3b: Stärken/Schwächen (je 3 Zeilen × 4) → 7 Minuten
- **Realistisch: Ja**

**Aufgabe 4 (6 Punkte / ~6 Minuten):**
- Eigene Position (12 Zeilen) → 6 Minuten
- **Realistisch: Ja**

**Gesamtzeit: 45 Minuten** [PASSED]

Die Test-Struktur ist für 45 Minuten konzipiert und realistisch machbar. Schüler müssen konzentriert, aber nicht übereilt arbeiten.

---

## Bewertungsskala (15-Punkte-System Sachsen Oberstufe)

| Erreichte Punkte | Notenpunkte | Note | Bewertung |
|-----------------|-------------|------|-----------|
| 43-45 | 15 | 1+ | Sehr gut |
| 41-42 | 14 | 1 | Sehr gut |
| 39-40 | 13 | 1- | Sehr gut |
| 36-38 | 12 | 2+ | Gut |
| 34-35 | 11 | 2 | Gut |
| 32-33 | 10 | 2- | Gut |
| 29-31 | 9 | 3+ | Befriedigend |
| 27-28 | 8 | 3 | Befriedigend |
| 25-26 | 7 | 3- | Befriedigend |
| 23-24 | 6 | 4+ | Ausreichend |
| 21-22 | 5 | 4 | Ausreichend |
| 19-20 | 4 | 4- | Ausreichend |
| 16-18 | 3 | 5+ | Mangelhaft |
| 14-15 | 2 | 5 | Mangelhaft |
| 12-13 | 1 | 5- | Mangelhaft |
| 0-11 | 0 | 6 | Ungenügend |

---

## Allgemeine Korrekturhinweise

**Positive Bewertung bei:**
- Verwendung korrekter philosophischer Fachbegriffe
- Strukturierte, logische Argumentation
- Systematische Anwendung der Theorien auf den Fall
- Eigenständige, kritische Reflexion
- Berücksichtigung verschiedener Perspektiven

**Abzüge bei:**
- Fehlender oder falscher Fachterminologie
- Unsystematischer Argumentation
- Bloßer Wiedergabe ohne Anwendung
- Gravierenden sachlichen Fehlern

**Teilpunkte großzügig vergeben bei:**
- Richtiger Grundrichtung, unvollständiger Ausführung
- Kleinen Ungenauigkeiten bei sonst korrekter Darstellung
- Guten Ansätzen, die nicht vollständig zu Ende gedacht sind

---

**Ende des Erwartungshorizonts**

**Viel Erfolg bei der Korrektur!**
